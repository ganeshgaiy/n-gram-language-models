{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bored-graduation",
   "metadata": {
    "id": "bored-graduation"
   },
   "source": [
    "# CS.7389J HW 1 \n",
    "# Coding (C) Section: Language Model and Naive Bayes Classifier\n",
    "\n",
    "This notebook contains the coding (C) section of HW 1. \n",
    "\n",
    "First you will build word and character level n-gram language models for Yelp reviews and the language models to generate sample  reviews. Second, you will build Naive Bayes classifiers to distinguish between legitimate news headlines and clickbait.\n",
    "\n",
    "After this assignment, you will learn to\n",
    "1. train ngram language models given a text corpus;\n",
    "2. generate text from a language model;\n",
    "3. calculate the probability of text given a language model;\n",
    "4. classify news headlines using Naive Bayes;\n",
    "5. evaluate classifiers on a test set.\n",
    "\n",
    "**Do not edit this notebook.**  Write your code in ```language_model.py``` and ```naive_bayes.py```.\n",
    "\n",
    "## Setup\n",
    "Run the following cell to upgrade your NLTK version to the latest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-large",
   "metadata": {
    "id": "angry-large"
   },
   "outputs": [],
   "source": [
    "!pip3 install nltk --upgrade  # after running this line once, you can comment this line out\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "print(nltk.__version__) # this should print out 3.8.1 if you have installed the latest version of NLTK properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ec2d2",
   "metadata": {
    "id": "6e6ec2d2"
   },
   "source": [
    "Use the function below to autoreload extension functions in ```language_model.py``` and ```naive_bayes.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca6b2e",
   "metadata": {
    "id": "94ca6b2e"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from language_model import *\n",
    "from naive_bayes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-mason",
   "metadata": {
    "id": "potential-mason"
   },
   "source": [
    "## 1. N-gram Language Model [40 points]\n",
    "\n",
    "Train word-level and character-level (up to 4-ngrams) language models on Yelp reviews. The [dataset provided](https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset?resource=download) is a subset of [a larger dataset](https://www.yelp.com/dataset) published by Yelp.  Use the following function to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-particular",
   "metadata": {
    "id": "enormous-particular"
   },
   "outputs": [],
   "source": [
    "filename = 'yelp.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "all_text = df['text']\n",
    "\n",
    "trn_text, dev_text = train_test_split(all_text, test_size=0.2, random_state=42)\n",
    "trn_text, dev_text = trn_text.reset_index(drop=True), dev_text.reset_index(drop=True)\n",
    "print(\"trn_text:\")\n",
    "print(trn_text)\n",
    "print(\"\\ndev_text:\")\n",
    "print(dev_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-glance",
   "metadata": {
    "id": "latest-glance"
   },
   "source": [
    "### Data processing and n-gram counts [10 points]\n",
    "Train your language models on these reviews. Implement the class ```NGramLM``` and write a function called ```get_ngram_counts``` to process the reviews and get the counts of all n-grams in the reviews. Store n-grams in the dictionary `self.ngram_count`. This dictionary will contain other dictionaries as values. For instance, `self.ngram_count[0]` will be a dictionary containing all of the unigrams, and `self.ngram_count[1]` will be a dictionary containing all the bigrams. \n",
    "\n",
    "Use the following rules when processing the reviews:\n",
    "- Prepend **two/three** &lt;s&gt; at the beginning of each review as BOS (begining of sentence) tokens (two for trigram model and three for four-gram model), and append one &lt;/s&gt; at the end of a review as the EOS (end of sentence) token.\n",
    "- Convert all letters to lowercase.\n",
    "- Tokenize each review. Do not split BOS and EOS tokens\n",
    "- Replace all unigrams that occur < 2 times with \"UNK\" \n",
    "- You will need to make the function ```get_ngram_counts``` flexible enough so that (1) it can operate on both the character-level and the word-level and (2) it can operate on a variety of maximum ngram sizes (as determined by `ngram_size`)\n",
    "- Do **NOT** remove punctuation.\n",
    "\n",
    "Test your functions using the following code: \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-sympathy",
   "metadata": {
    "id": "superb-sympathy"
   },
   "outputs": [],
   "source": [
    "bos_token, eos_token = '<s>', '</s>'\n",
    "ngram_size = 3 # Use trigrams\n",
    "word_lm = NGramLM(bos_token, eos_token, word_tokenize, ngram_size)\n",
    "word_lm.get_ngram_counts(trn_text.tolist())\n",
    "print(f\"Number of unigrams: {len(word_lm.ngram_count[0])}\")\n",
    "least_unigram = min(word_lm.ngram_count[0].keys(), key=lambda x: word_lm.ngram_count[0][x])\n",
    "print(f\"Unigram with smallest count: {least_unigram}\\tCount: {word_lm.ngram_count[0][least_unigram]}\")\n",
    "print(f\"Unknown unigram: {word_lm.ngram_count[0]['UNK']}\")\n",
    "print(f\"Number of BOS token: {word_lm.ngram_count[0][bos_token]}\")\n",
    "print(f\"Number of bigrams: {len(word_lm.ngram_count[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef26743",
   "metadata": {
    "id": "3ef26743"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-giant",
   "metadata": {
    "id": "ethical-giant"
   },
   "outputs": [],
   "source": [
    "def char_tokenizer(text):\n",
    "    return [char for char in text]\n",
    "    \n",
    "ngram_size = 4 # Use four-grams\n",
    "char_lm = NGramLM(bos_token, eos_token, char_tokenizer, ngram_size)\n",
    "char_lm.get_ngram_counts(trn_text.tolist())\n",
    "print(f\"Number of unigrams: {len(char_lm.ngram_count[0])}\")\n",
    "least_unigram = min(char_lm.ngram_count[0].keys(), key=lambda x: char_lm.ngram_count[0][x])\n",
    "print(f\"Unigram with smallest count: {least_unigram}\\tCount: {char_lm.ngram_count[0][least_unigram]}\")\n",
    "print(f\"Unknown unigram: {char_lm.ngram_count[0]['UNK']}\")\n",
    "print(f\"Number of BOS token: {char_lm.ngram_count[0][bos_token]}\")\n",
    "print(f\"Number of bigrams: {len(char_lm.ngram_count[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-charge",
   "metadata": {
    "id": "peripheral-charge"
   },
   "source": [
    "### Add-k Smoothing [5 points]\n",
    "Write the function ```add_k_prob``` so that given a bigram $(w_1, w_2)$, a unigram $w_3$, and $k$, return $p(w_3|(w_1, w_2))$ after applying add-k smoothing.\n",
    "Notes:\n",
    "- Program this flexibly enough so that, for the character-level model, the model can smooth over trigrams and unigrams rather than bigrams and unigrams.\n",
    "- &lt;s&gt; should **NOT** be considered when calculating the vocabulary size because it will never be generated by the language model (although it's in ```self.unigram_count```). &lt;/s&gt; and \"UNK\" should both be treated as tokens in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-trust",
   "metadata": {
    "id": "substantial-trust"
   },
   "outputs": [],
   "source": [
    "# prepare counts\n",
    "test_data = ['An apple', 'A circle', 'This dot', 'That triangle', 'The red apple', 'The red circle', 'The blue dot', 'The blue triangle']\n",
    "k = 0.5\n",
    "test_word_lm = NGramLM(bos_token, eos_token, word_tokenize, 3)\n",
    "test_word_lm.get_ngram_counts(test_data)\n",
    "print(f\"Vocabulary: {list(test_word_lm.ngram_count[0].keys())}\")\n",
    "# test\n",
    "bigram = ('the', 'red')\n",
    "unigram = 'apple'\n",
    "print(f\"Probability of seen: {test_word_lm.add_k_smooth_prob(bigram, unigram, k)}\")\n",
    "\n",
    "bigram = ('the', 'blue')\n",
    "unigram = 'apple'\n",
    "print(f\"Probability of unseen: {test_word_lm.add_k_smooth_prob(bigram, unigram, k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1040d",
   "metadata": {
    "id": "4bf1040d"
   },
   "source": [
    "Now test out the same smoothing code on the character-level model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-status",
   "metadata": {
    "id": "unexpected-status"
   },
   "outputs": [],
   "source": [
    "test_data = ['plan', 'plant', 'planet']\n",
    "k = 0.5\n",
    "test_char_lm = NGramLM(bos_token, eos_token, char_tokenizer, 4)\n",
    "test_char_lm.get_ngram_counts(test_data)\n",
    "print(f\"Vocabulary: {list(test_char_lm.ngram_count[0].keys())}\")\n",
    "# test\n",
    "trigram = ('p', 'l', 'a')\n",
    "unigram = 'n'\n",
    "print(f\"Probability of seen: {test_char_lm.add_k_smooth_prob(trigram, unigram, k)}\")\n",
    "\n",
    "trigram = ('p', 'l', 'a')\n",
    "unigram = 't'\n",
    "print(f\"Probability of unseen: {test_char_lm.add_k_smooth_prob(trigram, unigram, k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1e619-b0d5-486e-bde5-90d2a2682d9e",
   "metadata": {
    "id": "yellow-respondent"
   },
   "source": [
    "### Linear interpolation [4 points]\n",
    "\n",
    "Write the function ```linear_interp_prob``` so that given a bigram $(w_1, w_2)$, a unigram $w_3$, and list of values [$\\lambda_1$, $\\lambda_2$, $\\lambda_3$], return $p(w_3|(w_1, w_2))$ after applying linear interpolation.\n",
    "\n",
    "Implement this flexibly enough to operate on four-grams for the character-level model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-summit",
   "metadata": {
    "id": "latest-summit"
   },
   "outputs": [],
   "source": [
    "lambda1 = 0.6\n",
    "lambda2 = 0.2\n",
    "lambda3 = 0.2\n",
    "lambdas = [lambda1, lambda2, lambda3]\n",
    "\n",
    "bigram = ('the', 'red')\n",
    "unigram = 'apple'\n",
    "print(f\"Probability of seen: {test_word_lm.linear_interp_prob(bigram, unigram, lambdas)}\")\n",
    "\n",
    "bigram = ('the', 'blue')\n",
    "unigram = 'apple'\n",
    "print(f\"Probability of unseen: {test_word_lm.linear_interp_prob(bigram, unigram, lambdas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86807e66",
   "metadata": {
    "id": "86807e66"
   },
   "source": [
    "Now test ```linear_interp_prob``` on the character-level model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-debut",
   "metadata": {
    "id": "accessible-debut"
   },
   "outputs": [],
   "source": [
    "lambda1 = 0.6\n",
    "lambda2 = 0.2\n",
    "lambda3 = 0.1\n",
    "lambda4 = 0.1\n",
    "lambdas = [lambda1, lambda2, lambda3, lambda4]\n",
    "\n",
    "trigram = ('p', 'l', 'a')\n",
    "unigram = 'n'\n",
    "print(f\"Probability of seen: {test_char_lm.linear_interp_prob(trigram, unigram, lambdas)}\")\n",
    "\n",
    "trigram = ('p', 'l', 'a')\n",
    "unigram = 't'\n",
    "print(f\"Probability of unseen: {test_char_lm.linear_interp_prob(trigram, unigram, lambdas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-cambridge",
   "metadata": {
    "id": "front-cambridge"
   },
   "source": [
    "### Calculate next word probability [2 points]\n",
    "Write the function ```get_probability``` that calculates $p(w_3|(w_1, w_2))$ using either add-k smoothing or linear interpolation that you implemented above. The input is a dictionary that specifies how should you do the smoothing. O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-delivery",
   "metadata": {
    "id": "trying-delivery"
   },
   "outputs": [],
   "source": [
    "smoothing_args1 = {\n",
    "    'method': 'add_k',\n",
    "    'k': 0.5\n",
    "}\n",
    "smoothing_args2 = {\n",
    "    'method': 'linear',\n",
    "    'lambdas': [0.6, 0.2, 0.2]\n",
    "}\n",
    "bigram = ('a', 'red')\n",
    "unigram = 'dot'\n",
    "print(f\"Add-k smoothing: {test_word_lm.get_probability(bigram, unigram, smoothing_args1)}\")\n",
    "print(f\"Linear interpolation: {test_word_lm.get_probability(bigram, unigram, smoothing_args2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-singer",
   "metadata": {
    "id": "multiple-singer"
   },
   "source": [
    "### Calculate perplexity [4 points]\n",
    "Write the function ```get_perplexity``` so that given a document and smoothing arguments, return the perplexity of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-examination",
   "metadata": {
    "id": "hawaiian-examination"
   },
   "outputs": [],
   "source": [
    "text = \"This sentence contains unseen words.\"\n",
    "\n",
    "for word in text.split():\n",
    "    if word in test_word_lm.ngram_count[0].keys():\n",
    "        count = test_word_lm.ngram_count[0][word]\n",
    "    else:\n",
    "        count = 0\n",
    "    print(f\"Count of {word} is {count}\")\n",
    "\n",
    "print(f\"Add-k smoothing: {test_word_lm.get_perplexity(text, smoothing_args1)}\")\n",
    "print(f\"Linear interpolation: {test_word_lm.get_perplexity(text, smoothing_args2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-correlation",
   "metadata": {
    "id": "coupled-correlation"
   },
   "source": [
    "### Search hyperparameters [6 points]\n",
    "First find the best k value for the word-level model and for the character-level model using add-k smoothing. You need to search k in this list: \\[0.2, 0.4, 0.6, 0.8, 1.0\\].\n",
    "\n",
    "Write the function ```search_k``` such that given a validation set, return the best k value on it. Print out the perplexity (average on the whole validation set) for each k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-dealer",
   "metadata": {
    "id": "acceptable-dealer"
   },
   "outputs": [],
   "source": [
    "print(\"Word LM\")\n",
    "word_k = word_lm.search_k(dev_text)\n",
    "print(f\"Best k: {word_k}\")\n",
    "print(\"Char LM\")\n",
    "char_k = char_lm.search_k(dev_text)\n",
    "print(f\"Best k: {char_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-theorem",
   "metadata": {
    "id": "hydraulic-theorem"
   },
   "source": [
    "Write the function ```search_lambda``` such that, given a validation set, returns the best $\\lambda$ values on it.  \n",
    "Note: this code block might take a couple minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-monaco",
   "metadata": {
    "id": "blocked-monaco"
   },
   "outputs": [],
   "source": [
    "print(\"Word LM\")\n",
    "word_lambda = word_lm.search_lambda(dev_text)\n",
    "print(\"Char LM\")\n",
    "char_lambda = char_lm.search_lambda(dev_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9616083",
   "metadata": {
    "id": "d9616083"
   },
   "source": [
    "### Generate reviews [5 points]\n",
    "Write the function ```generate_text``` to generate a sentence based on an input prompt. To generate the text, find the distribution of the next word given previous two words (or next character given the previous three characters). Then  **randomly** sample the next word/character based on the distribution (i.e., do **NOT** use greedy decoding, which is deterministically choosing the most probable unigram). After the word/character is sampled, append it to the current text and continue generating the next word/character. Repeat this process until the current sequence **has 15 words/characters** (including prompts) or you **generate the &lt;/s&gt; token**.\n",
    "\n",
    "Note that more advanced methods to generate text from language models exist, such as beam search, top-k sampling, and top-p sampling. You can refer to [this blog](https://huggingface.co/blog/how-to-generate) for more information. In this assignment, you are not required to implement the advanced methods, sampling from the trigram/four-gram distribution would be enough. However, you can use this notebook to explore if you'd like to.\n",
    "\n",
    "Begin with the word-level model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d1e55",
   "metadata": {
    "id": "cb9d1e55"
   },
   "outputs": [],
   "source": [
    "prompts = [['The', 'location'], ['We', 'ate'], ['I', 'thought'], ['It', 'had']]\n",
    "for prompt in prompts:\n",
    "    print(\"Word LM\")\n",
    "    word_lm.generate_text(prompt, smoothing_args1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b7b51a",
   "metadata": {
    "id": "49b7b51a"
   },
   "source": [
    "Now test the character-level model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774fc2f",
   "metadata": {
    "id": "0774fc2f"
   },
   "outputs": [],
   "source": [
    "prompts = ['The store', 'We ate']\n",
    "for prompt in prompts:\n",
    "    print(\"Char LM\")\n",
    "    prompt_tokenized = char_tokenizer(prompt)\n",
    "    char_lm.generate_text(prompt_tokenized, smoothing_args1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-dining",
   "metadata": {
    "id": "swiss-dining"
   },
   "source": [
    "### Predict the class of a review [4 points]\n",
    "The Yelp review dataset contains several columns specifying attributes of each review:\n",
    "- ```stars``` indicates on a scale from 1 to 5 how many stars the reviewer gave in the review\n",
    "- ```cool```, ```useful```, and ```funny``` all indicate how many users rated the review as cool, useful or funny\n",
    "\n",
    "- Write the function ```load_new_data``` to generate a split in the data. For instance, one class could be reviews with at least one funny rating and the opposing class could be reviews not marked as funny by anyone. In this function, use the techniques at the beginning of the notebook to split the overall dataset into two classes of your choice\n",
    "- Write function ```predict_class``` to predict the class to which the review in ```test_review.txt``` belongs. This function will take in two word-level language models (each trained on the training data for one class) and compute the probability of the review generated by each language model. I.e., $p(W|LM_1)$ and $p(W|LM_2)$, or equivalently $PPL(W)$ based on $LM_1$ and $LM_2$. Print out the perplexity of each language model and your prediction.\n",
    "\n",
    "The test review used has the following attributes:\n",
    "- ```stars``` : 2\n",
    "- ```useful``` : 3\n",
    "- ```funny``` : 7\n",
    "- ```cool``` : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-craft",
   "metadata": {
    "id": "aware-craft"
   },
   "outputs": [],
   "source": [
    "filename = 'yelp.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "smoothing_args = {\n",
    "    'method': 'linear',\n",
    "    'lambdas': word_lambda\n",
    "}\n",
    "\n",
    "class1_data, class2_data = load_new_data(df)\n",
    "\n",
    "class1_lm = NGramLM(bos_token, eos_token, word_tokenize, 3)\n",
    "class1_lm.get_ngram_counts(class1_data)\n",
    "\n",
    "class2_lm = NGramLM(bos_token, eos_token, word_tokenize, 3)\n",
    "class2_lm.get_ngram_counts(class2_data)\n",
    "\n",
    "predict_class('test_review.txt', class1_lm, class2_lm, smoothing_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-poison",
   "metadata": {
    "id": "regular-poison"
   },
   "source": [
    "## 2. Naive Bayes for Text Classification [26 points]\n",
    "Implement a Naive Bayes classifiers using the clickbait headlines dataset, which contains examples of legitimate news headlines and clickbait news headlines. The original dataset can be found in [this GitHub repository](https://github.com/bhargaviparanjape/clickbait) and [this paper](https://arxiv.org/abs/1610.09786).\n",
    "\n",
    "### C.2.1 Load dataset [4 points]\n",
    "Write the function ```load_headlines``` to load the clickbait dataset into pandas dataframes. Use the file ```clickbait_data.csv``` \n",
    "\n",
    "1. Read in the ```text``` and ```is_clickbait``` columns.\n",
    "2. Rename the ```is_clickbait``` column to ```label```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-execution",
   "metadata": {
    "id": "realistic-execution"
   },
   "outputs": [],
   "source": [
    "from naive_bayes_answer import *\n",
    "\n",
    "all_data = load_headlines('clickbait_data.csv')\n",
    "\n",
    "(train, test) = train_test_split(all_data, train_size=0.9)\n",
    "\n",
    "display(train)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-spank",
   "metadata": {
    "id": "earlier-spank"
   },
   "source": [
    "### C.2.2 Dataset statistics [3 points]\n",
    "Write the function ```get_basic_stats``` to print out the following statistics of the training data:\n",
    "- Average number of tokens per headline\n",
    "- Standard deviation of the number of tokens per headline\n",
    "- Total number of legitimate headlines\n",
    "- Total number of clickbait headlines\n",
    "\n",
    "Note: you can use any tokenization method you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-account",
   "metadata": {
    "id": "sticky-account"
   },
   "outputs": [],
   "source": [
    "get_basic_stats(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-rental",
   "metadata": {
    "id": "exterior-rental"
   },
   "source": [
    "### C.2.3 Data processing and ngram calculation [6 points]\n",
    "Write the function ```fit``` to calculate n-grams, given a dataframe of training data, calculates the ngram counts in each category and the prior probability for each category.  Follow these rules when calculating the counts:\n",
    "- convert all letters to lowercase;\n",
    "- include both unigrams and bigrams;\n",
    "- ignore terms that appear in more than 80\\% of the headlines;\n",
    "- ignore terms that appear in less than 3 headlines.\n",
    "\n",
    "Hint: use ```CountVectorizer``` in sklearn and store it as ```self.vectorizer```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-opening",
   "metadata": {
    "id": "personalized-opening"
   },
   "outputs": [],
   "source": [
    "naive_bayes = NaiveBayes()\n",
    "naive_bayes.fit(train)\n",
    "print(f\"Probability for each category: {naive_bayes.category_prob}\")\n",
    "print(f\"Length of self.ngram_count: {len(naive_bayes.ngram_count)}\")\n",
    "print(f\"Shape of the counts for 1st category: {naive_bayes.ngram_count[0].shape}\")\n",
    "print(f\"Number of non-zero terms for 1st category: {(naive_bayes.ngram_count[0] > 0).sum()}\")\n",
    "print(f\"Maximum count of the 1st category: {naive_bayes.ngram_count[0].max()}\")\n",
    "print(f\"Minimum count of the 1st category: {naive_bayes.ngram_count[0].min()}\")\n",
    "print(f\"Sum of ngram count for 1st category: {naive_bayes.ngram_count[0].sum()}\")\n",
    "print(f\"Total count for each category: {naive_bayes.total_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-decimal",
   "metadata": {
    "id": "blind-decimal"
   },
   "source": [
    "### Calculate posterior probability for a category [4 points]\n",
    "Write the function ```calculate_prob``` that given a list of articles $docs$, a category index $i$, return $\\log\\left(p(c_i)p(d|c_i)\\right)=\\log\\left(p(c_i)\\prod_{x\\in X}p(x|c_i)\\right)$ for each article $d$ in $docs$, where $X$ is the set of unigrams and bigrams in **both** article $d$ and vocabulary $V$.\n",
    "\n",
    "- Use **add-one smoothing** in your calculation.\n",
    "- Simply discard unseen unigrams/bigrams (do not use add-one smoothing to account for them).\n",
    "- Calculate the **sum of logarithms** to avoid issues with underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-determination",
   "metadata": {
    "id": "brave-determination"
   },
   "outputs": [],
   "source": [
    "test_docs = [\"United Kingdom officially exits the European Union\",\n",
    " \"How to Lose a Guy in 10 Days\"]\n",
    "prob1 = naive_bayes.calculate_prob(test_docs, 0)\n",
    "prob2 = naive_bayes.calculate_prob(test_docs, 1)\n",
    "print(f\"Probability for category 0: {prob1}\")\n",
    "print(f\"Probability for category 1: {prob2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-sheep",
   "metadata": {
    "id": "occupied-sheep"
   },
   "source": [
    "### Predict labels for new headlines [2 points]\n",
    "Write the function ```predict``` that, given a list of headlines, returns the predicted categories of the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-territory",
   "metadata": {
    "id": "opposed-territory"
   },
   "outputs": [],
   "source": [
    "preds = naive_bayes.predict(test_docs)\n",
    "print(f\"Prediction: {preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-semester",
   "metadata": {
    "id": "bronze-semester"
   },
   "source": [
    "### Calculate evaluation metrics [5 points]\n",
    "Write he function ```evaluate```in which given a list of predictions and a list of true labels, returns the accuracy, macro f1-score, and micro f1-score. Do not use functions in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-casino",
   "metadata": {
    "id": "level-casino"
   },
   "outputs": [],
   "source": [
    "predictions = [1,1,0,1,0,0,1]\n",
    "labels = [1,0,0,1,0,1,1]\n",
    "accuracy, mac_f1, mic_f1 = evaluate(predictions, labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Macro f1: {mac_f1}\")\n",
    "print(f\"Micro f1: {mic_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-corner",
   "metadata": {
    "id": "warming-corner"
   },
   "source": [
    "### Test classifier on test data [2 points]\n",
    "Test your classifier Run the following cell to make predictions and print out performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-angle",
   "metadata": {
    "id": "amended-angle"
   },
   "outputs": [],
   "source": [
    "predictions = naive_bayes.predict(test.text.tolist())\n",
    "labels = test.label.tolist()\n",
    "accuracy, mac_f1, mic_f1 = evaluate(predictions, labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Macro f1: {mac_f1}\")\n",
    "print(f\"Micro f1: {mic_f1}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
